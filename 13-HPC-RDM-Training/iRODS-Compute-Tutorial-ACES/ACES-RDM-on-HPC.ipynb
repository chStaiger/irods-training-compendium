{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDM in an advanced data pipeline on an HPC cluster\n",
    "## Introduction\n",
    "In this tutorial are working through parts of the computational pipeline in https://doi.org/10.3389/fgene.2013.00289.\n",
    "The pipeline deals with the problem to find out the best configuration for predicting the outcome of breast cancer patients based on their gene expression profiles. One major aspect in the performance of the classification is the selection of features. Several methods to select genes or groups of genes are compared in a double-loop cross validation procedure. For simplicity in this tutorial we only focus on two benchmark methods to select genes (Single genes and Random genes) and one method, Lee, which uses pathways to group genes and classfify then with the aggregated statistics over these genes.\n",
    "We will also only use one classifier model, the neares mean classifier with the standard metric. And to keep running times down we will also only execute a 5-fold cross validation.\n",
    "\n",
    "The experssion dataset is published on figshare and we will download it during the pipeline from this external source. The curated pathway data, which is necessary for the Lee method to define features, lies in the iRODS instance and is annotated by some metadata. \n",
    "\n",
    "## Step-by-step guide\n",
    "### 1. Install dependencies\n",
    "Before we start the computationa pipeline, we need to make sure that all necessary python modules are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade sklearn python-irodsclient wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Imports\n",
    "Here we import some standard python modules, the irods python modules, some own functions to ease the interaction with iRODS and of course our own software ACES, which implements the datatypes and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Standard python modules\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import numpy\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import wget\n",
    "import pprint\n",
    "\n",
    "#iRODS python modules\n",
    "from irods.session import iRODSSession\n",
    "from irods.models import Collection, DataObject, CollectionMeta, DataObjectMeta\n",
    "from irods.access import iRODSAccess\n",
    "#Own python functions to wrap lengthy iRODS code\n",
    "from helperFunctions import *\n",
    "\n",
    "#Imports from the ACES software\n",
    "from SetUpGrid import CombineData\n",
    "from CreateTokens import generate_tokens\n",
    "from SetUpGrid import SetUpRun\n",
    "from SetUpGrid import RunInstance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Connection to iRODS\n",
    "Here we define the parameters which are needed to connect to iRODS and setup the connection:\n",
    "\n",
    "**Please adjust the user, password and share!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Connect to iRODS')\n",
    "host = \"<IP or FQDN>\"\n",
    "port = 1247\n",
    "user = \"<irods user>\"\n",
    "password = \"<irods password>\"\n",
    "zone = \"aliceZone\"\n",
    "share = \"<another irods user or group>\" # share the output with this other iRODS user or group\n",
    "session = iRODSSession(host=host, port=port, user=user, password=password, zone=zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parameters for our computational pipeline\n",
    "- Setting up the folder structure on fast storage of the compute cluster. The data stored here is not backed up, nor safely stored, this storage is just used to allow very quick calculations on the data.\n",
    "- Keywords and their values to search for the correct data in iRODS\n",
    "- Prepare a collection on the iRODS instance to gather our results\n",
    "\n",
    "**Please adjust the paths!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDir = \"/lustre/scratch/GUESTS/<user>/acesdata\"\n",
    "resultsDir = \"/lustre/scratch/GUESTS/<user>/acesresults\"\n",
    "ensure_dir(dataDir)\n",
    "ensure_dir(resultsDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can search for our curated pathway data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ATTR_NAME = 'DATATYPE'\n",
    "ATTR_VALUE = 'PATHWAYS'\n",
    "query = session.query(Collection.name, DataObject.name)\n",
    "filteredQuery = query.filter(DataObjectMeta.name == ATTR_NAME).\\\n",
    "                          filter(DataObjectMeta.value == ATTR_VALUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(filteredQuery.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And download the data to our directory on the scratch file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iPaths = iParseQuery(filteredQuery)\n",
    "print('Downloading: ')\n",
    "print('\\n'.join(iPaths))\n",
    "iGetList(session, iPaths, dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still lack the gene expression dataset. Let us download it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://ndownloader.figshare.com/files/4851460\"\n",
    "fileName = \"4851460\"\n",
    "wget.download(url, out=dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the computational pipeline we will create output data which we would like to directly store in iRODS. So we need to create a dedicated collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coll = session.collections.get('/' + zone + '/home/' +user)\n",
    "collNames = [c.name for c in coll.subcollections]\n",
    "resultsName = 'aces_results'\n",
    "tmp = resultsName\n",
    "count = 0\n",
    "while resultsName in collNames:\n",
    "        resultsName = tmp + '_' +str(count)\n",
    "        count = count + 1\n",
    "print('Upload results to: '+ coll.path + '/' + resultsName)\n",
    "coll = session.collections.create(coll.path + '/' + resultsName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The ACES pipeline\n",
    "Now that we have setup our environment, downloaded all necessary data, we can progress and setup our data analysis pipeline.\n",
    "ACES does a huge parameter sweep by combining classifiers, feature selections methods, their parameters and the 5 splits of the dataset. Each parameter combination is defined in a 'token'. From these tokens the run is initialised and results are created.\n",
    "But first let's create all combinations between the dataset and the feature selection algorithms and their parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DataAndFeatureExtractors = CombineData()\n",
    "DataAndFeatureExtractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these combinations we need to create tokens that define how our gene expression dataset is split, on which splits the classifier is trained and which split serves as testing dataset. Let's do that for **the first** of the items in our CombineData list.\n",
    "- 'dataset': name of the dataset\n",
    "- 'fold': the fold of the data that serves as testing data\n",
    "- 'method': Feature selector algorithm\n",
    "- 'network': Gene interaction network or pathways data name\n",
    "- 'repeat': 0 (we do not repeat the 5 fold cross validation)\n",
    "- 'ShuffleNr': Used to randomised/shuffle the 'network' data (not used in this tutorial)\n",
    "- 'specific': Set to extra values for the 'method' if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item = DataAndFeatureExtractors[0]\n",
    "tokens = generate_tokens([item], 1, 5, \"PerfTest\")\n",
    "pprint.pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading all necessary data, pathways etc into memory:\n",
    "(data, net, featureSelector, classifiers, Dataset2Time) = \\\n",
    "        SetUpRun(item[0], item[1][1], item[1][0][0], datafile = \"4851460\", datapath=dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the classification on each of the tokens.Four of the folds serve as training datasets. On that training dataset we rank the features according to their discriminative power and then subsequently adding them to the Nearest Mean Classifier. The classification performance is evaluated by Area-uner-the-Receiver-Operator-Curve (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "        dataset = token['input']['dataset']\n",
    "        network = token['input']['network']\n",
    "        method = token['input']['method']\n",
    "        repeat = token['input']['repeat']\n",
    "        fold = token['input']['fold']\n",
    "        print('dataset:', dataset)\n",
    "        print('network', network)\n",
    "        print('method', method)\n",
    "        print('repeat', repeat)\n",
    "        print('fold', fold)\n",
    "        (dataName, featureExtractorproductName, netName, shuffle, featureExtractor, AucAndCi) = RunInstance(\n",
    "                    data, net, featureSelector, None, classifiers, repeat, 5, fold, None, Dataset2Time, None)\n",
    "        token['output'] = (dataName, featureExtractorproductName, netName, None, shuffle, \n",
    "                           featureExtractor, AucAndCi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pprint(token['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the performances. We see the best-ranked feature gives us aleardy an AUC of 0.71:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(list(token['output'][6]['BinaryNearestMeanClassifier_V1'].items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(optional)** But how does this best feature look like, i.e. which genes were used in that feature? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fe = json.loads(token['output'][5])\n",
    "type(fe)\n",
    "fe[0]\n",
    "print(\"name:\", fe[0])\n",
    "print(\"space (list of all known genes):\")\n",
    "pprint.pprint(fe[1][:10])\n",
    "print(\"best ranked featues (index of gene):\")\n",
    "pprint.pprint(fe[2][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the Entrez number of the genes in the first feature, one needs to look the index up in the space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature = fe[2][1]\n",
    "[fe[1][idx] for idx in feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Send the results to iRODS\n",
    "Let us save the list of tokens to a file and put it into iRODS. we will directly stream the data ino an iRODS object rather than first creating a file on the scratch file system and subsequently uploading the file to iRODS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filebase = item[0]+'_'+item[1][0][0]+'_'+item[1][1]+'_raw.json' # name of the file\n",
    "obj = session.data_objects.create(coll.path + \"/\" + filebase) # Create a new data object in iRODS\n",
    "print(\"Data will be written to iRODS:\", obj.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with obj.open('w') as obj_desc:\n",
    "    obj_desc.write(json.dumps(tokens).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify**: Open a browser and go to https://<IP or FQDN>/aliceZone/home/<user\\>/aces_results<_num\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create some metadata and summarising plot for your co-worker (share)\n",
    "As you have seen, the raw data is very hard to parse for human beings. However, we would like to give the person or group we defined in 'share' some impression ofd the data. Hence, we will create some metadata that captures the provenance, i.e. how the data came into being, and some plots. Let's start with the metadata: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.metadata.add('ISEARCH', ATTR_NAME + '==' + ATTR_VALUE)\n",
    "obj.metadata.add('ISEARCHDATE', str(datetime.date.today()))\n",
    "obj.metadata.add('prov:wasDerivedFrom', 'http://dx.doi.org/10.6084/m9.figshare.3119248.v1')\n",
    "obj.metadata.add('DATATYPE', 'ACES results')\n",
    "obj.metadata.add('prov:SoftwareAgent', 'ACES')\n",
    "obj.metadata.add('ALGORITHM', filebase.split('_raw.json')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some summarising data and upload it to iRODS. We will create a performance figure and extract the 50 best scoring features per split of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []\n",
    "for token in tokens:\n",
    "    performance.append([token['output'][6]['BinaryNearestMeanClassifier_V1'][perf][0]\n",
    "        for perf in list(token['output'][6]['BinaryNearestMeanClassifier_V1'].keys())[:50]])\n",
    "    \n",
    "plt.plot(numpy.transpose(performance))\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('AUC (performance)')\n",
    "plt.title(token['output'][1]+' '+str(token['output'][2]))\n",
    "figName = 'performance_'+filebase+'.png'\n",
    "plt.savefig(resultsDir+'/'+figName)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Write plot to iRODS: '+coll.path+'/'+figName)\n",
    "session.data_objects.put(resultsDir+'/'+figName, coll.path+'/'+figName)\n",
    "obj = session.data_objects.get(coll.path+'/'+figName)\n",
    "obj.metadata.add('REFDATA', 'http://dx.doi.org/10.6084/m9.figshare.3119248.v1')\n",
    "obj.metadata.add('DATATYPE', 'ACES results')\n",
    "obj.metadata.add('ALGORITHM', filebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 50 most differentially expressed features\n",
    "bestFeatures = []\n",
    "for token in tokens:\n",
    "    _, genes, features = json.loads((token['output'][5]))\n",
    "    if item[1][1] != None:\n",
    "        genelist = [genes[feat] for sublist in features[:10] for feat in sublist]\n",
    "    else:\n",
    "        genelist = features[:10]\n",
    "    bestFeatures.append(genelist)\n",
    "csvName = 'features_'+filebase+'.csv'\n",
    "with open(resultsDir+'/'+csvName, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerows(bestFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to iRODS\n",
    "print('Write feature csv to iRODS: '+coll.path+'/'+csvName)\n",
    "session.data_objects.put(resultsDir+'/'+csvName, coll.path+'/'+csvName)\n",
    "obj = session.data_objects.get(coll.path+'/'+csvName)\n",
    "obj.metadata.add('REFDATA', 'http://dx.doi.org/10.6084/m9.figshare.3119248.v1')\n",
    "obj.metadata.add('DATATYPE', 'ACES results')\n",
    "obj.metadata.add('ALGORITHM', filebase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify**: Go back to your browser and check Verify: Open a browser and go to https://<IP or FQDN>/aliceZone/home/<your user\\>/aces_results\\<_tag\\>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Share the data\n",
    "We indicated a person or a group with which we want to share the data. Now we will adjust the accession to our iROPDS results folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coll.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for srcColl, colls, objs in coll.walk():\n",
    "    for obj in objs:\n",
    "        try:\n",
    "            acl = iRODSAccess('read', obj.path, share, session.zone)\n",
    "            session.permissions.set(acl)\n",
    "        except:\n",
    "            print(\"User or group unknown: \"+share)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Last step: Clean up\n",
    "To free storage for other users let us clean up the scratch file system. All of our result data is in iRODS, so we will not need them any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up: \", dataDir)\n",
    "print(\"Cleaning up: \", resultsDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "print(\"Removing local data in\", dataDir)\n",
    "rmtree(dataDir)\n",
    "print(\"Removing local data in\", resultsDir)\n",
    "rmtree(resultsDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "We have seen that parameter sweeps can become quite elaborate. You might want to think beforehand which output do I really need for my analysis and interpretation.\n",
    "Instead of running everything in many nested for-loops we defined tokens that decode the parameter setting of a run.\n",
    "\n",
    "We also saw that the calculation of only one combination of data and algorithm can take some time. So we want to setup the whole pipeline and run it remotely (not interactively) on an HPC cluster.\n",
    "The tokenisation of the single parameter combinations also gives us the chance to start several jobs each considering a different set of tokens. The results however will be gathered in iRODS no matter where the tokens are calculated.\n",
    "\n",
    "Next step: Run the pipeline on the HPC ... see you there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.9.4",
   "language": "python",
   "name": "python3.9.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
